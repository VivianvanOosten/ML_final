---
title: "Vivian van Oosten : AM11 Individual Assignment Part 2: Text Mining + PCA"
output:
  html_document:
    theme: cosmo
    highlight: haddock
    toc: yes
    toc_float: yes
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE}
library(knitr)
library(tidyverse)
library(data.table)
library(tm) # package for text mining  
library(SnowballC) # for stemming words
library(stringr) # package to count number of words in a string
#library(RWeka) # package for ngrams

knitr::opts_chunk$set(
  tidy = FALSE,   # display code as typed
  size = "small",# slightly smaller font for code
  warning = FALSE,
  message = FALSE) 
```

# Introduction

The tags users give to the movies they rate can be very informative. Tags like 'great' or 'terrible' can help us predict the rating and ultimately determine what movies we want to watch. We can also use the tags to group the movies and see which ones are similar.

In this document we will preprocess the tags data to form a document-term matrix. This matrix will then be used in a Principal Component Analysis to remove any variance that we do not need and reduce the matrix to fewer dimensions. The goal is to prepare the data to use the principal components in machine learning analysis in the future. 


# Data Preprocessing

```{r Read Data}

# reading in the data
tags <- fread('ml-25m/tags.csv') %>% select(movieId, tag)
# 1093360 by 2

head(tags)

# creating a dataframe tb with all tags for each movie pasted in a single string
# and alss making it all lowercase
tb <- tags %>%
  group_by(movieId) %>%
  summarise(tag = paste0(tag, collapse =" ") )
# 45251 by 2

# filtering for at least 100 words
# by counting the number of spaces, which is the number of 
# words in the string. Then we filter to have only movies with 100 or more words
tb <- tb %>%
  mutate(count = str_count(tag, " ") ) %>%
  filter(count > 99) %>%
  select(-count)
# 2918 by 2

# removing special characters from the string
# special characters are defined as any character that
# is not an alphanumeric character
tb <- tb %>%
  mutate(tag = str_replace_all(tag, "[^[:alnum:] ]", ""))
# 2918 by 2 

head(tb)

```

# Text Mining 

The preprocessing we are using consists of three parts. Firstly, we remove all special characters and make everything lowercase. This leaves us with just numbers and lowercase letters, which we can process further. Second, we will remove all stopwords. These are words such as 'the', 'as', and 'in'. These do not add any information to our data but could be used regularly for all movies. Lastly, we stem all of the entries. This will ensure that words are brought back to their 'stem', which for example turns loved lover loving lovely all into love. Because they all represent the same idea, it's best to combine them into one word and not flood our analysis with many different words. 


```{r Text Mining}

# Creates the corpus we will be using to make the DTM
tb <- tb %>%
  rename_at('movieId', ~'doc_id') %>%
    rename_at('tag', ~'text') %>%
  mutate(doc_id = as.character(doc_id))
x <- Corpus(VectorSource(tb$text))

```

```{r}
# making everything lower-case, remove stop words,
# remove punctuation, removing excess whitespace,
# removing numbers and stemming the document
# in that order
x <- tm_map(x, tolower) 
x <- tm_map(x,removeWords, stopwords("english")) 
x <- tm_map(x,removePunctuation)
#x <- tm_map(x, stripWhitespace) 
x <- tm_map(x,removeNumbers)
x <- tm_map(x,stemDocument, language = "english")

```

```{r}
# creating the document term matrix
DTM <- DocumentTermMatrix(x)

# First we see what the document term matrix looks like without removing sparse terms
m <- as.matrix(DTM)
DTM_tbl <- as_tibble(m) 
DTM_tidy <- pivot_longer(DTM_tbl, cols = everything(), names_to = "word", values_to = "wordCount")
DTM_tidy %>%
  group_by(word) %>%
  summarise(wordCount = sum(wordCount)) %>%
  
# plotting the word-count frequencies 
ggplot(aes(x = wordCount)) +
  geom_histogram(bins = 40) +
  theme_bw() + 
  labs(
    title = 'Distribution of word-count is very skewed',
    y = '# words with count',
    x = 'Word count'
  )


```

Most words have a very very low word count, which means they were only used in very few words. We do not want to keep these extremely sparse terms in our matrix, because they do not add any information. We would also not be able to generalise based on so few instances of the word.

However, we will remove sparse words that occur in less than 2% of the movies. We set this limit very high, because our PCA will already remove the unnecessary variance, which includes very sparse terms that are uninformative. Therefore, we can keep them in case there is information that we need. 

```{r}
# now removing sparse terms from the DTM and redoing the creation of the dataframe
DTM <- removeSparseTerms(DTM, 0.98) 
movieIds <- seq(1, nrow(tb), 1)
rownames(DTM) <- movieIds

m <- as.matrix(DTM)
DTM_tbl <- as_tibble(m) 
DTM_tidy <- pivot_longer(DTM_tbl, cols = everything(), names_to = "word", values_to = "wordCount")

print(head(DTM_tbl))
print(head(DTM_tidy))

```


# Exploratory Analysis

Before we start the PCA, we explore the data a little. 

First, we print the words with the highest and lowest frequency. This gives us an idea of the kind of words that could end up being important in our analysis.


```{r}
# we check the word frequency of highest and lowest words
DTM_tidy %>%
  group_by(word) %>%
  summarise(wordCount = sum(wordCount)) %>%
  arrange(wordCount) %>%
  head(10)
```

The words with the lowest frequency are kaf and congress. Interestingly, these words appear to not make a lot of sense. They also only occur in max 60 out of 700 documents, so they are not very important. It could be that these are titles or concepts in a specific movie and therefore only mentioned for that movie or movieseries. 


```{r}
DTM_tidy %>%
  group_by(word) %>%
  summarise(wordCount = sum(wordCount)) %>%
  arrange(-wordCount) %>%
  head(10)
```

The words with the highest frequency are comedy, end and sci-fi. Considering those can describe a large number of movies, this is not a surprising result. 


```{r}
DTM_tidy %>%
  group_by(word) %>%
  summarise(wordCount = sum(wordCount)) %>%
  
ggplot(aes(x = wordCount)) +
  geom_histogram(bins = 40) +
  theme_bw() + 
  labs(
    title = 'Distribution of word-count is right skewed',
    y = '# words with count',
    x = 'Word count'
  )

```

Most words have a word-count bewteen 0 and 2500. It's likely that these words will hold the most information. However, some words, such as 'great', have been used very frequently and can also be very informative for the rating of that movie. Therefore, we do not want to use TF-IDF to weight the words with. The amount that a word occurs for other movies does not influence the importance that the word has for one specific movie in this case. 


# Principle Component Analysis

In order to further reduce the dimensionality of the data and create a dataset we could comfortable use for SVM, we run a PCA on the document term matrix. We started out with 699 columns in our Document Term Matrix. 

In our preprocessing, we center the data to 0 but we do not scale it, since scaling the data makes the PCA perform much worse. With scaling, it only explains 10% of the variance with 5 columns, while without scaling it explains over 40%. This is also related to our decision not to use tfifd, since scaling the data would perform a similar role. 

```{r PCA}
dim(DTM_tbl) # 2918 699

DTM_tbl <- scale(DTM_tbl, center = TRUE, scale = FALSE)
pca <- prcomp(DTM_tbl, center = FALSE, scale. = FALSE) 
# printing the importance of the first 50 components
summary(pca)$importance[, 1:50]

```

The table shows part of the story, but to choose the optimal number of columns, we use the below visualisation of variance explained as well.

```{r}
VE <- pca$sdev^2
PVE <- VE / sum(VE) * 100
CPVE <- cumsum(PVE)
df <- data.frame(PC = c(1:699),
                 var_explained = VE,
                 cum_sum_PVE = CPVE)

# --- VARIANCE EXPAINED (SCREEPLOT), VE:
M <- 699
df[1:M,] %>% # select M PCs (rows) and all columns of the dataframe to pass into ggplot
  ggplot(aes(x = PC, y = var_explained)) +
  geom_point(size = 0.5) +
  geom_line() + 
  geom_hline(yintercept = 1, linetype = "dashed", color = "red", size = 1.2) +
  labs(x = "PC Number", y = "VE", 
       title = "Scree Plot", 
       subtitle = "PCA on movies data") +
  theme(axis.text = element_text(size = 15),
        axis.title = element_text(size = 15, face = "bold", color = "blue"),
        plot.title = element_text(size = 18, color = "blue"),
        plot.subtitle = element_text(size = 15, color = "blue") )
```

Clearly the additional relevance of columns decreases very quickly. Below we take a closer look at the plot to determine the exact dimensions we want to take. We select a PC number between 0 and 150, since somewhere before there the elbow occurs that we want to take. 

```{r}
# --- VARIANCE EXPAINED (SCREEPLOT), VE:
M <- 699
df[1:M,] %>% # select M PCs (rows) and all columns of the dataframe to pass into ggplot
  ggplot(aes(x = PC, y = var_explained)) +
  geom_point(size = 0.5) +
  geom_line() + 
  scale_x_continuous(limits = c(0, 150) ) +
  scale_y_continuous(limits = c(0, 200)) + 
  geom_hline(yintercept = 1, linetype = "dashed", color = "red", size = 1.2) +
  labs(x = "PC Number", y = "VE", 
       title = "Scree Plot", 
       subtitle = "PCA on movies data") +
  theme(axis.text = element_text(size = 15),
        axis.title = element_text(size = 15, face = "bold", color = "blue"),
        plot.title = element_text(size = 18, color = "blue"),
        plot.subtitle = element_text(size = 15, color = "blue") )
```

Based on this plot, we will keep 25 columns. This is approximately the point where the curve levels off, and each additional column adds less information from there on out. This set of components will explain 69% of the variance in the data, as we can see in the table above. This is well over half, which is quite good given that we went from 699 variables to 25. 

We select these features so that we could use them in the analysis from here on out. 

```{r}
# selecting only the features we want
pca_features <- pca$x[, 1:25]

# to use the features later, we have saved them to an RDS object
#saveRDS(pca_features, 'part2_pca_features_object.rds')
```

