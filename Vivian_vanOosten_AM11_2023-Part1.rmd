---
title: "YOUR NAME : AM11 Individual Assignment Part 1: Recommendation Systems"
output:
  html_document:
    theme: cosmo
    highlight: haddock
    toc: yes
    toc_float: yes
  pdf_document:
    toc: yes
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
library(knitr)
library(tidyverse)
library(skimr)
library(recommenderlab)
library(ggplot2)                       
library(data.table)
library(janitor)
library(reshape2)


knitr::opts_chunk$set(
  tidy = FALSE,   # display code as typed
  size = "small") # slightly smaller font for code
```


# Step 0: Data cleaning

We load the data and inspect it to see what our columns are. We have a column for user ID, movie ID, rating and a timestamp. This means that every individual rating is a row in our dataframe, with the corresponding values. 

```{r}
ratings <- fread('ml-25m/ratings.csv')
movies <- fread('ml-25m/movies.csv', stringsAsFactors=FALSE)
```

```{r}
# exploring the movie ratings dataset
skimr::skim(ratings)
skimr::skim(movies)
```
```{r}
# checking for duplicate values
length(unique(ratings$userId))
length(unique(ratings$movieId))

length(unique(movies$movieId))
length(unique(movies$title))
# theres a difference between the number of IDs (higher) and the number of titles (lower)
```

```{r, eval=FALSE}
# remove this bit - the bit below does all we need it to do
# getting the duplicates and removing those ratings from the ratings doc
duplicate_movies <- movies %>%
  get_dupes(title) %>%
  distinct(title, .keep_all = TRUE) %>%
  select(movieId)

movies_without_dupes <- movies %>%
  filter(! (movieId %in% duplicate_movies$movieId ) )

ratings_without_dupes <- ratings %>%
  filter(! (movieId %in% duplicate_movies$movieId ) )
```



```{r}
repeatMovies <- names(which(table(movies$title) > 1))
removeRows <- integer()
for(i in repeatMovies){
  repeatMovieLoc <- which(movies$title == i)
  tempGenre <- paste(movies$genres[repeatMovieLoc], collapse="|")
  tempGenre <- paste(unique(unlist(strsplit(tempGenre, split = "\\|")[[1]])), collapse = "|")
  movies$genres[repeatMovieLoc[1]] <- tempGenre
  
  ##### REMOVE REPEATS IN RATING DATA ####
  repeatMovieIdLoc <- which(ratings$movieId %in% movies$movieId[repeatMovieLoc[-1]])
  ratings$movieId[repeatMovieIdLoc] <- movies$movieId[repeatMovieLoc[1]]
  removeRows <- c(removeRows, repeatMovieLoc[-1])
}
movies$movieId[removeRows]
movies <- movies[-removeRows,]
movies[movies$title == repeatMovies[1],]
movies[movies$title == repeatMovies[2],]
rm(i, removeRows, repeatMovieIdLoc, repeatMovieLoc, repeatMovies, tempGenre)
```

Next, we need to remove any user that has rated a single movie twice.
We only keep the highest rating if they have rated a single movie twice. 
```{r}
ratings <- ratings[sample(1:nrow(ratings), 0.05*nrow(ratings)), ]

duplicate_ratings_keep <- ratings %>% 
  get_dupes(userId, movieId) %>%
  group_by(userId, movieId) %>%
  summarise(rating = max(rating)) 
  
ratings <- ratings %>%
  # completely removing all duplicates
  group_by(userId, movieId) %>% 
  filter(n()==1) %>%
  # adding the desired duplicates back in
  left_join(duplicate_ratings_keep)

```

Our dataset now consists of unique movies and unique ratings. 

```{r}
#SELECTING ONLY SMALL DATASET
#movies <- movies[sample(1:nrow(movies), 0.05*nrow(movies)), ]
#ratings <- ratings %>%
#  filter(movieId %in% movies$movieId)

#ratings <- ratings[sample(1:nrow(ratings), 0.05*nrow(ratings)), ]

```


# Step 1: Exploratory Analysis


To investigate the distribution of the ratings of our dataset, we make 3 histograms. Our first histogram shows every individual rating as an observation in the histogram. The second and third take the average ratings of movies and users respectively, rounded to half points, and plots those.

The general histogram below shows that users are more likely to choose whole points for their ratings than half points, with 4 being the most common rating and 0.5 the least common. We see a left skewed distribution. In general, a relatively positive rating (3+) is much more common than a negative rating (less than 3). 

```{r eval=FALSE}
ggplot(data = ratings, mapping = aes(x=rating)) +
  geom_bar() +
  labs(
    title = 'Integer values are the most frequent ratings',
    x = 'Rating',
    y = 'Count'
  ) + 
  theme_bw()
```

Next we plot the average ratings of each movie. We see that most movies have an average rating of around 3, with very few movies having 1 or 5 as an average rating. 

```{r eval=FALSE}
ratings %>%
  group_by(movieId) %>%
  summarise(rating = mean(rating)) %>%
  mutate(rating = round(rating*2)/2) %>%
  ungroup() %>%
ggplot(aes(x=rating)) +
  geom_bar() +
  labs(
    title = 'Movies average rating distribution is skewed left',
    x = 'Average Rating',
    y = 'Count'
  ) + 
  theme_bw()
```



```{r eval=FALSE}
ratings %>%
  group_by(userId) %>%
  summarise(rating = mean(rating)) %>%
  mutate(rating = round(rating*2)/2) %>%
  ungroup() %>%
ggplot(aes(x=rating)) +
  geom_bar() +
  labs(
    title = 'The average ratings of users are 3.5',
    x = 'Average Rating',
    y = 'Count'
  ) + 
  theme_bw()
```

# Step 2: Data Engineering

## Selecting popular movies

We need to base our recommendations on data, so movies that have been rated by fewer than 20 users will not be included in our final database. We do not know enough about these movies. 

```{r eval=FALSE}
m <- 20
movies_at_least_m <- ratings %>%
  group_by(movieId) %>%
  filter(n() >= m)

```

## Selecting popular users

A similar reasoning counts for our users. We need to know what users like before we can recommend anything to them. Therefore, we will not use users who have rated fewer than 50 movies. 

```{r eval=FALSE}
n <- 50
users_at_least_n <- movies_at_least_m %>%
  group_by(userId) %>%
  filter(n() >= n)
```

We will not be using these created dataframes, since we want to be able to adjust the filtering to see how that affects the performance of our recommendation systems. 

# Step 3: Model build

We build three recommendations systems: item based, user based and model based. 



Turning the ratings into a matrix
```{r}
library(Matrix)

movies <- unique(ratings$movieId)
users <- unique(ratings$userId)

ratings$col <- match(ratings$movieId, movies)
ratings$row <- match(ratings$userId, users)

df_sparse <- sparseMatrix(
  i = ratings$row, 
  j = ratings$col,
  x = ratings$rating,
  dimnames = list(users, movies),
  repr = 'C'
)

dim(df_sparse)
# [1] 145461   2933
```

```{r}
# defining a function that removes the bottom users and movies
selecting_fav_movies <- function(df, m, n) {
  # number of nonzero per column
  xx <- diff(df@p)
 # print(df@p)
  
  # number of nonzero per row
  yy <- tabulate(df@i +1)
  
  # filtering for the bottom users and movies
  df <- df[df[yy > m]]
  df <- df[df[xx > n]]
  
  df
}

```



```{r eval=FALSE}
recommendation_model <- recommenderRegistry$get_entries(dataType = "realRatingMatrix")
names(recommendation_model)

lapply(recommendation_model, "[[", "description")

recommendation_model$IBCF_realRatingMatrix$parameters
```


```{r }
m <- c(10,20,50,100,200)
n <- c(10,20,50,100,200)
modelName <- c('IBCF', 'UBCF', 'LIBMF')
search_grid <- expand_grid(m,n,modelName)

#create data frame with 0 rows and 3 columns
results <- data.frame(matrix(ncol = 6, nrow = 0))

#provide column names
colnames(results) <- c('m', 'n', 'modelName', 'RMSE', 'MSE', 'MAE')

for (m_temp in m){
  for (n_temp in n){
    for (modelName_temp in modelName){
      
      print(m_temp)
      print(n_temp)
      print(modelName_temp)
    
    movies_at_least_m <- ratings %>%
      group_by(movieId) %>%
      filter(n() >= m_temp) %>%
      select(movieId) %>%
      unique()
    
    users_at_least_n <- ratings %>%
      group_by(userId) %>%
      filter(n() >= n_temp) %>%
      select(userId) %>%
      unique()
        
   ratings_temp <- ratings %>%
     filter(
       userId %in% users_at_least_n$userId,
       movieId %in% movies_at_least_m$movieId
     )
   
    print("Created temp")
      
    movies <- unique(ratings_temp$movieId)
    users <- unique(ratings_temp$userId)
    
    ratings_temp$col <- match(ratings_temp$movieId, movies)
    ratings_temp$row <- match(ratings_temp$userId, users)
    
    df_sparse <- sparseMatrix(
      i = ratings_temp$row, 
      j = ratings_temp$col,
      x = ratings_temp$rating,
      dimnames = list(users, movies),
      repr = 'C'
    )
      
    print("Created sparse")
      
  df_temp <- as(df_sparse, "realRatingMatrix")
  #df_temp <- df_sparse[rowCounts(df_sparse) > n_temp,
                            #  colCounts(df_sparse) > m_temp]
  
  if (modelName_temp == 'IBCF'){
    parameters = list(k = 350)
  } else if (modelName_temp == 'UBCF'){
    parameters = list(nn=10)
  } else if (modelName_temp == 'LIBMF') {
    parameters = list(dim = 10)
  }
  

  e <- evaluationScheme(df_temp, method="split", train=0.8, given=-5)
    
  RMSE.model <- Recommender(getData(e, "train"), method = modelName_temp, 
                          parameter = parameters) # normalise = center
  
  print("Initial recomender done")
  
  prediction <- predict(object = RMSE.model, newdata = getData(e, "known") , type="ratings")
  
  row <- calcPredictionAccuracy(x = prediction, data = getData(e, "unknown"))
  
  row$m <- m_temp
  row$n <- n_temp
  row$modelName <- modelName_temp
  
  results <- rbind(results, row)
  
}
}
}
  
  
  
```

```{r}
saveRDS(results, 'part1_results_object.rds')
```


```{r}
# plotting the results in a nice graph
results %>% 
  filter(n == 50) %>%
ggplot(aes(x = m, y = RMSE, color = modelName)) + 
  geom_line() +
  theme_bw() +
  labs(
    title = 'Model based CFs perform the best',
    color = 'Model type'
  )
```

```{r}
# plotting the results in a nice graph
results %>% 
  filter(m == 50) %>%
ggplot(aes(x = n, y = RMSE, color = modelName)) + 
  geom_line() +
  theme_bw() +
  labs(
    title = 'Model based CFs perform the best',
    color = 'Model type'
  )
```

```{r}
library(viridis)
# plotting the results in a nice graph
results %>% 
  mutate(m = as.factor(m),
         n = as.factor(n)) %>%
ggplot(aes(x = m, y = n, fill = RMSE)) + 
  facet_wrap(~modelName) + 
  geom_tile() +
  scale_fill_viridis(direction = -1) +
  theme_bw() +
  labs(
    title = 'Model based CFs perform the best',
    color = 'RMSE'
  )
```


# Conclusion

The different models perform vastly differently on same data and the same combination of m and n. This shows clearly that the different methods can be applied to different situations. 
In our case, the model based CF with matrix factorization performed better than the other item&user based models. The matrix factorization simplifies the users and movies we are using by aggregating similar ones into 'superusers' and 'supermovies'. These represent a group of similar entries. While user-item interactions are very sparse, the supergroups have a clearer idea of the interaction between groups and are able to smooth out rare events. Given the very sparse data we have, it is not very surprising that such an approach works best of all.
Our best model has m = 50, n = 50 and uses the LIMBF model. 


```{r eval=FALSE}
saveRDS(training_data, file = "part1_training_data_object.rds")

saveRDS(testing_data, file = "part1_testing_data_object.rds")

saveRDS(df_sparse, file = "part1_df_sparse_object.rds")

```


```{r eval=FALSE}
training_data <- readRDS( "part1_training_data_object.rds")

testing_data <- readRDS("part1_testing_data_object.rds")

df_sparse <- readRDS("part1_df_sparse_object.rds")


```