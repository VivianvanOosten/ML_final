---
title: "Vivian van Oosten : AM11 Individual Assignment Part 1: Recommendation Systems"
output:
  html_document:
    theme: cosmo
    highlight: haddock
    toc: yes
    toc_float: yes
  pdf_document:
    toc: yes
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
library(knitr)
library(tidyverse)
library(skimr)
library(recommenderlab)
library(ggplot2)                       
library(data.table)
library(janitor)
library(reshape2)


knitr::opts_chunk$set(
  tidy = FALSE,   # display code as typed
  size = "small") # slightly smaller font for code
```


# Introduction

One of the recommendation systems used most regularly is the one used by various streaming services, such as Netflix and HBO. They usually have a section specifically made to recommend movies to their customers. They use some form of recommendation algorithm to do this for their customers. 

We have received a dataset of movies and their ratings by a lot of users. With this data, we want to create the best recommendation system we can. To do so, we will first clean and prepare the data and then compare three different recommendation systems. These three include a user-user system, an item-item system and a model-based matrix factorization system. The models will be explained more deeply in the assignment. 


# Step 0: Data cleaning

We load the data and inspect it to see what our columns are. We have a column for user ID, movie ID, rating and a timestamp. This means that every individual rating is a row in our dataframe, with the corresponding values. 

```{r}
ratings <- fread('ml-25m/ratings.csv')
movies <- fread('ml-25m/movies.csv', stringsAsFactors=FALSE)
```

```{r}
# exploring the movie ratings dataset
skimr::skim(ratings)
skimr::skim(movies)
```

We check to see if there are any double entries in the ratings and movie datasets. Keeping these doubles would give a higher weight to the users and/or movies that have double entries for no good reason. Therefore, we will remove them in the next block of code.

```{r}
# checking for duplicate values
length(unique(ratings$userId))
length(unique(ratings$movieId))

length(unique(movies$movieId))
length(unique(movies$title))
# theres a difference between the number of IDs (higher) and the number of titles (lower)
```


```{r}
repeatMovies <- names(which(table(movies$title) > 1))
removeRows <- integer()
for(i in repeatMovies){
  repeatMovieLoc <- which(movies$title == i)
  tempGenre <- paste(movies$genres[repeatMovieLoc], collapse="|")
  tempGenre <- paste(unique(unlist(strsplit(tempGenre, split = "\\|")[[1]])), collapse = "|")
  movies$genres[repeatMovieLoc[1]] <- tempGenre
  
  ##### REMOVE REPEATS IN RATING DATA ####
  repeatMovieIdLoc <- which(ratings$movieId %in% movies$movieId[repeatMovieLoc[-1]])
  ratings$movieId[repeatMovieIdLoc] <- movies$movieId[repeatMovieLoc[1]]
  removeRows <- c(removeRows, repeatMovieLoc[-1])
}
movies$movieId[removeRows]
movies <- movies[-removeRows,]
movies[movies$title == repeatMovies[1],]
movies[movies$title == repeatMovies[2],]
rm(i, removeRows, repeatMovieIdLoc, repeatMovieLoc, repeatMovies, tempGenre)
```

Next, we need to remove any user that has rated a single movie twice. We only keep the highest rating if they have rated a single movie twice. This choice is made because rating the movie twice likely indicates that they liked the movie enough to watch it twice. We could have also taken the average or the latest rating, but made this choice in this context. 

```{r}
ratings <- ratings[sample(1:nrow(ratings), 0.05*nrow(ratings)), ]

duplicate_ratings_keep <- ratings %>% 
  get_dupes(userId, movieId) %>%
  group_by(userId, movieId) %>%
  summarise(rating = max(rating)) 
  
ratings <- ratings %>%
  # completely removing all duplicates
  group_by(userId, movieId) %>% 
  filter(n()==1) %>%
  # adding the desired duplicates back in
  left_join(duplicate_ratings_keep)

```

Our dataset now consists of unique movies and unique ratings. It is ready for some exploratory analysis. 


# Step 1: Exploratory Analysis


To investigate the distribution of the ratings of our dataset, we make 3 histograms. Our first histogram shows every individual rating as an observation in the histogram. The second and third take the average ratings of movies and users respectively, rounded to half points, and plots those.

The general histogram below shows that users are more likely to choose whole points for their ratings than half points, with 4 being the most common rating and 0.5 the least common. We see a left skewed distribution. In general, a relatively positive rating (3+) is much more common than a negative rating (less than 3). 

```{r}
ggplot(data = ratings, mapping = aes(x=rating)) +
  geom_bar() +
  labs(
    title = 'Integer values are the most frequent ratings',
    x = 'Rating',
    y = 'Count'
  ) + 
  theme_bw()
```

Next we plot the average ratings of each movie. We see that most movies have an average rating of around 3, with very few movies having 1 or 5 as an average rating. 

```{r}
ratings %>%
  group_by(movieId) %>%
  summarise(rating = mean(rating)) %>%
  mutate(rating = round(rating*2)/2) %>%
  ungroup() %>%
ggplot(aes(x=rating)) +
  geom_bar() +
  labs(
    title = 'Movies average rating distribution is skewed left',
    x = 'Average Rating',
    y = 'Count'
  ) + 
  theme_bw()
```

Lastly we plot the average rating that each user gives. We see that there is a bit of a difference between the opinions of users on general. Since a rating system is not an objective measure, this could mean that there is a difference in interpretation of the metric. Some users could interpret 4 as 'alright' and some interpret a 4 as 'great!'. This difference can show itself in a different average rating. Therefore, we need to center and scale the data to be able to see the real opinion of each user on a specific movie. 
We could be wrong, there could be users that only give a rating if a movie is very bad, which would affect their average rating as well. However, then the difference between the ratings of each movie is still the only relevant metric. All movies are bad does not help us identify the movies that the user liked. By taking the difference with the mean, we can still see which movies they liked more and which movies they liked less and thus give them the best possible recommendation we can. 

```{r}
ratings %>%
  group_by(userId) %>%
  summarise(rating = mean(rating)) %>%
  mutate(rating = round(rating*2)/2) %>%
  ungroup() %>%
ggplot(aes(x=rating)) +
  geom_bar() +
  labs(
    title = 'The average ratings of users are 3.5',
    x = 'Average Rating',
    y = 'Count'
  ) + 
  theme_bw()
```

# Step 2: Data Engineering

In this section, we prepare the data for use in the recommendation systems. 

## Selecting popular movies

We need to base our recommendations on data, so movies that have been rated by fewer than X users will not be included in our final database. We do not know enough about these movies. 

```{r eval=FALSE}
m <- 20
movies_at_least_m <- ratings %>%
  group_by(movieId) %>%
  filter(n() >= m)

```

## Selecting popular users

A similar reasoning counts for our users. We need to know what users like before we can recommend anything to them. Therefore, we will not use users who have rated fewer than X movies. 

```{r eval=FALSE}
n <- 50
users_at_least_n <- movies_at_least_m %>%
  group_by(userId) %>%
  filter(n() >= n)
```

In further sections of this model, we do not use these exact objects since we want to identify the exact X that provides the best performance. 

# Step 3: Model build

We build three recommendations systems: item based, user based and model based and iterate over the different values for m and n that we have identified.

## User-User model
A model like this will assess similarity between users based on their ratings of movies. We will consider users who have high and low ratings for the same movies to be similar. When looking for a movie to recommend to a user, we look for the users that are their nearest neighbours. Then, we find the movie that most of these neighbours rated highly that our user has not watched yet. This is the movie or set of movies we then recommend. 

## Item-Item based
Item-item is very similar to the user-user, but more appropriate for a case where we have fewer items and many users. With item-item, we assess the similarity between items based on how many of the same users like this item. If a user then liked or used a specific item, we can find similar items and recommend those to the user. We can also further extend this model by considering baskets of items instead of individual items, so that we can recommend an item from a different basket.

## Model-based with Matrix Factorization
Especially when we have very large amounts of data, it makes sense to use a matrix-factorization model. Matrix factorization takes users and items, and creates supergroups of both categories. These supergroups consist of similar users or items and allow us to compress our giant very sparse user-item-matrix into a smaller denser one. With a denser matrix, we are better able to make inferences based on the data because we simply have relatively more data available. 


```{r eval=FALSE}
m <- c(10,20,50,100)
n <- c(10,20,50,100)
modelName <- c('IBCF', 'UBCF', 'LIBMF')
search_grid <- expand_grid(m,n,modelName)

#create data frame with 0 rows and 3 columns
results <- data.frame(matrix(ncol = 6, nrow = 0))

#provide column names
colnames(results) <- c('m', 'n', 'modelName', 'RMSE', 'MSE', 'MAE')

for (m_temp in m){
  for (n_temp in n){
    for (modelName_temp in modelName){
      
      print(m_temp)
      print(n_temp)
      print(modelName_temp)
    
    movies_at_least_m <- ratings %>%
      group_by(movieId) %>%
      filter(n() >= m_temp) %>%
      select(movieId) %>%
      unique()
    
    users_at_least_n <- ratings %>%
      group_by(userId) %>%
      filter(n() >= n_temp) %>%
      select(userId) %>%
      unique()
        
   ratings_temp <- ratings %>%
     filter(
       userId %in% users_at_least_n$userId,
       movieId %in% movies_at_least_m$movieId
     )
   
    print("Created temp")
      
    movies <- unique(ratings_temp$movieId)
    users <- unique(ratings_temp$userId)
    
    ratings_temp$col <- match(ratings_temp$movieId, movies)
    ratings_temp$row <- match(ratings_temp$userId, users)
    
    df_sparse <- sparseMatrix(
      i = ratings_temp$row, 
      j = ratings_temp$col,
      x = ratings_temp$rating,
      dimnames = list(users, movies),
      repr = 'C'
    )
      
    print("Created sparse")
      
  df_temp <- as(df_sparse, "realRatingMatrix")
  #df_temp <- df_sparse[rowCounts(df_sparse) > n_temp,
                            #  colCounts(df_sparse) > m_temp]
  
  if (modelName_temp == 'IBCF'){
    parameters = list(k = 10)
  } else if (modelName_temp == 'UBCF'){
    parameters = list(nn=10)
  } else if (modelName_temp == 'LIBMF') {
    parameters = list(dim = 10)
  }
  

  e <- evaluationScheme(df_temp, method="split", train=0.8, given=-5)
    
  RMSE.model <- Recommender(getData(e, "train"), method = modelName_temp, 
                          parameter = parameters) # normalise = center
  
  print("Initial recomender done")
  
  prediction <- predict(object = RMSE.model, newdata = getData(e, "known") , type="ratings")
  
  row <- calcPredictionAccuracy(x = prediction, data = getData(e, "unknown"))
  
  row$m <- m_temp
  row$n <- n_temp
  row$modelName <- modelName_temp
  
  results <- rbind(results, row)
  
}
}
}
  
  
  
```

Because running the code above takes a long time, we have ran it once and saved our results in the object below. Instead of running it again, we simply load in the object for the knitting of this file. 

```{r }
#saveRDS(results, 'part1_results_object.rds')
results <- readRDS('part1_results_object.rds')
```

Below, we plot the results of our for loop in a few different ways. We set n and m to 50 and plot the other one in the next two plots.

```{r}
# plotting the results in a nice graph
results %>% 
  filter(n == 50) %>%
ggplot(aes(x = m, y = RMSE, color = modelName)) + 
  geom_line() +
  theme_bw() +
  labs(
    title = 'Model based CFs perform the best',
    color = 'Model type'
  )
```


```{r}
# plotting the results in a nice graph
results %>% 
  filter(m == 50) %>%
ggplot(aes(x = n, y = RMSE, color = modelName)) + 
  geom_line() +
  theme_bw() +
  labs(
    title = 'Model based CFs perform the best',
    color = 'Model type'
  )
```

In both plots, we clearly observe a performance difference between the three models. Clearly, the model based recommendation system with matrix factorization performs the best. In the graphs below, we can see that we have a very sparse matrix. Most users only watched very few movies and most movies were only watched by very few users. The matrix factorization model makes this matrix much more dense by defining supergroups of users and of items. This additional density means that we have more information on each supergroup and are better able to make inferences based on that information. Therefore, it is not surprising that this model performs the best in this extremely sparse case. 


```{r}
ratings %>%
  group_by(userId) %>%
  summarise(n = n()) %>%
ggplot(aes(x = n)) +
  geom_histogram() + 
  theme_bw() + 
  labs(
    title = 'Most users watched only a few movies',
    x = 'Number of movies watched',
    y = 'Number of users'
  )
```

```{r}
ratings %>%
  group_by(movieId) %>%
  summarise(n = n()) %>%
ggplot(aes(x = n)) +
  geom_histogram() + 
  theme_bw() + 
  labs(
    title = 'Most movies were only watched a few times',
    x = 'Number of times watched',
    y = 'Number of movies'
  )
```

In an alternative visualisation, we show the RMSE of each model as the intensity of the colour in the plot below. This way we can visualise all combinations in one plot, and identify which types of combinations are the most efficient. We see that all the model-based models perform better than all the other models, and specifically using n = 50 for that model performs better than any other value of n, for all values of m.

```{r}
library(viridisLite)
library(viridis)
# plotting the results in a nice graph
results %>% 
  mutate(m = as.factor(m),
         n = as.factor(n)) %>%
ggplot(aes(x = m, y = n, fill = RMSE)) + 
  facet_wrap(~modelName) + 
  geom_tile() +
  scale_fill_viridis(direction = -1) +
  theme_bw() +
  labs(
    title = 'Model based CFs perform the best',
    color = 'RMSE'
  )
```


# Conclusion

The different models perform vastly differently on same data and the same combination of m and n. This shows clearly that the different methods can be applied to different situations. 
In our case, the model based CF with matrix factorization performed better than the other item&user based models. The matrix factorization simplifies the users and movies we are using by aggregating similar ones into 'superusers' and 'supermovies'. These represent a group of similar entries. While user-item interactions are very sparse, the supergroups have a clearer idea of the interaction between groups and are able to smooth out rare events. Given the very sparse data we have, it is not very surprising that such an approach works best of all.
Our best model has m = 50, n = 50 and uses the LIMBF model. 


To not have to rerun everything from the beginning, we have saved and reloaded our data objects. This is not run for the knitting, as it is irrelevant. 

```{r eval=FALSE}
saveRDS(training_data, file = "part1_training_data_object.rds")

saveRDS(testing_data, file = "part1_testing_data_object.rds")

saveRDS(df_sparse, file = "part1_df_sparse_object.rds")

```


```{r eval=FALSE}
training_data <- readRDS( "part1_training_data_object.rds")

testing_data <- readRDS("part1_testing_data_object.rds")

df_sparse <- readRDS("part1_df_sparse_object.rds")


```