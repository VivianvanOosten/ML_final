---
title: "YOUR NAME : AM11 Individual Assignment Part 2: Text Mining + PCA"
output:
  html_document:
    theme: cosmo
    highlight: haddock
    toc: yes
    toc_float: yes
  pdf_document:
    toc: yes
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
library(knitr)
library(tidyverse)
library(skimr)
library(recommenderlab)
library(ggplot2)                       
library(data.table)
library(janitor)
library(reshape2)


knitr::opts_chunk$set(
  tidy = FALSE,   # display code as typed
  size = "small") # slightly smaller font for code
```

# Assignment Completion Hints (DELETE THIS SECTION BEFORE SUBMITTING):

Hints for completing the assignment: - Ensure to document your code very
well explaining what your lines of code do - At every step that makes
sense, print out the current results in order to be able to see your
answers without running your code (i.e. the HTML knitted file should
show the results of that chunk of code) - For EVERY new matrix you
create during your work progress, print also their dimensions (nrows,
ncols) - For final R outputs and visualisations, elaborate also in
written form. - Before your final submission on canvas check whether
your RMD file compiles without errors. Submit RMD file and and its
knitted version HTML (or pdf) file. Make sure that all your outputs,
results and comments are visible in the knitted version.

Grading: The points split between the tasks below is: - 20% data
preprocessing - 40% text mining - 40% PCA

Further hints: - Avoid re-running joining tables several times because
it may change/affect the matrix dimension. Always run your code from
fresh if unsure (delete all variables and start from the top).

# Step 0: Data cleaning

We load the data and inspect it to see what our columns are. We have a column for user ID, movie ID, rating and a timestamp. This means that every individual rating is a row in our dataframe, with the corresponding values. 

```{r}
ratings <- fread('ml-25m/ratings.csv')
movies <- fread('ml-25m/movies.csv', stringsAsFactors=FALSE)
```

```{r}
# exploring the movie ratings dataset
skimr::skim(ratings)
skimr::skim(movies)
```
```{r}
# checking for duplicate values
length(unique(ratings$userId))
length(unique(ratings$movieId))

length(unique(movies$movieId))
length(unique(movies$title))
# theres a difference between the number of IDs (higher) and the number of titles (lower)
```

```{r, eval=FALSE}
# remove this bit - the bit below does all we need it to do
# getting the duplicates and removing those ratings from the ratings doc
duplicate_movies <- movies %>%
  get_dupes(title) %>%
  distinct(title, .keep_all = TRUE) %>%
  select(movieId)

movies_without_dupes <- movies %>%
  filter(! (movieId %in% duplicate_movies$movieId ) )

ratings_without_dupes <- ratings %>%
  filter(! (movieId %in% duplicate_movies$movieId ) )
```



```{r}
repeatMovies <- names(which(table(movies$title) > 1))
removeRows <- integer()
for(i in repeatMovies){
  repeatMovieLoc <- which(movies$title == i)
  tempGenre <- paste(movies$genres[repeatMovieLoc], collapse="|")
  tempGenre <- paste(unique(unlist(strsplit(tempGenre, split = "\\|")[[1]])), collapse = "|")
  movies$genres[repeatMovieLoc[1]] <- tempGenre
  
  ##### REMOVE REPEATS IN RATING DATA ####
  repeatMovieIdLoc <- which(ratings$movieId %in% movies$movieId[repeatMovieLoc[-1]])
  ratings$movieId[repeatMovieIdLoc] <- movies$movieId[repeatMovieLoc[1]]
  removeRows <- c(removeRows, repeatMovieLoc[-1])
}
movies$movieId[removeRows]
movies <- movies[-removeRows,]
movies[movies$title == repeatMovies[1],]
movies[movies$title == repeatMovies[2],]
rm(i, removeRows, repeatMovieIdLoc, repeatMovieLoc, repeatMovies, tempGenre)
```

Next, we need to remove any user that has rated a single movie twice.
We only keep the highest rating if they have rated a single movie twice. 
```{r}
duplicate_ratings_keep <- ratings %>% 
  get_dupes(userId, movieId) %>%
  group_by(userId, movieId) %>%
  summarise(rating = max(rating)) 
  
ratings <- ratings %>%
  # completely removing all duplicates
  group_by(userId, movieId) %>% 
  filter(n()==1) %>%
  # adding the desired duplicates back in
  left_join(duplicate_ratings_keep)

```

Our dataset now consists of unique movies and unique ratings. 


# Step 1: Exploratory Analysis


To investigate the distribution of the ratings of our dataset, we make 3 histograms. Our first histogram shows every individual rating as an observation in the histogram. The second and third take the average ratings of movies and users respectively, rounded to half points, and plots those.

The general histogram below shows that users are more likely to choose whole points for their ratings than half points, with 4 being the most common rating and 0.5 the least common. We see a left skewed distribution. In general, a relatively positive rating (3+) is much more common than a negative rating (less than 3). 

```{r}
ggplot(data = ratings, mapping = aes(x=rating)) +
  geom_bar() +
  labs(
    title = 'Integer values are the most frequent ratings',
    x = 'Rating',
    y = 'Count'
  ) + 
  theme_bw()
```

Next we plot the average ratings of each movie. We see that most movies have an average rating of around 3, with very few movies having 1 or 5 as an average rating. 

```{r}
ratings %>%
  group_by(movieId) %>%
  summarise(rating = mean(rating)) %>%
  mutate(rating = round(rating*2)/2) %>%
  ungroup() %>%
ggplot(aes(x=rating)) +
  geom_bar() +
  labs(
    title = 'Movies average rating distribution is skewed left',
    x = 'Average Rating',
    y = 'Count'
  ) + 
  theme_bw()
```



```{r}
ratings %>%
  group_by(userId) %>%
  summarise(rating = mean(rating)) %>%
  mutate(rating = round(rating*2)/2) %>%
  ungroup() %>%
ggplot(aes(x=rating)) +
  geom_bar() +
  labs(
    title = 'The average ratings of users are 3.5',
    x = 'Average Rating',
    y = 'Count'
  ) + 
  theme_bw()
```

# Step 2: Data Engineering

## Selecting popular movies

We need to base our recommendations on data, so movies that have been rated by fewer than 20 users will not be included in our final database. We do not know enough about these movies. 

```{r}
m <- 20
movies_at_least_m <- ratings %>%
  group_by(movieId) %>%
  filter(n() >= m)

```

## Selecting popular users

A similar reasoning counts for our users. We need to know what users like before we can recommend anything to them. Therefore, we will not use users who have rated fewer than 50 movies. 

```{r eval=FALSE}
n <- 50
users_at_least_n <- movies_at_least_m %>%
  group_by(userId) %>%
  filter(n() >= n)
```

We will not be using these created dataframes, since we want to be able to adjust the filtering to see how that affects the performance of our recommendation systems. 

# Step 3: Model build

We build three recommendations systems: item based, user based and model based. 



Turning the ratings into a matrix
```{r}
library(Matrix)

movies <- unique(ratings$movieId)
users <- unique(ratings$userId)

ratings$col <- match(ratings$movieId, movies)
ratings$row <- match(ratings$userId, users)

df_sparse <- sparseMatrix(
  i = ratings$row, 
  j = ratings$col,
  x = ratings$rating,
  dimnames = list(users,
                  movies)
)

dim(df_sparse)
# [1] 1000000   26000
```

```{r}
# defining a function that removes the bottom users and movies
selecting_fav_movies <- function(df, m, n) {
  df[rowCounts(df) > n, # at least n movies viewed by each user
        colCounts(df) > m] %>% # at least m users viewed each movie 
  normalize() 
}
```



```{r}
recommendation_model <- recommenderRegistry$get_entries(dataType = "realRatingMatrix")
names(recommendation_model)

lapply(recommendation_model, "[[", "description")

recommendation_model$IBCF_realRatingMatrix$parameters
```
```{r}
m <- 20
n <- 50

df_sparse <- selecting_fav_movies(df_sparse, m, n)
df_sparse <- as(df_sparse, "realRatingMatrix")
normalized_ratings <- normalize(df_sparse)
```

```{r}

```


