---
title: "YOUR NAME : AM11 Individual Assignment Part 2: Text Mining + PCA"
output:
  html_document:
    theme: cosmo
    highlight: haddock
    toc: yes
    toc_float: yes
  pdf_document:
    toc: yes
---

```{r setup, include=FALSE}
# Helper packages
library(dplyr)    # for data wrangling. A must for a data scientist :)
library(data.table)
library(tidyverse)
library(ggplot2)  # for awesome graphics
library(rsample)  # for efficient data splitting
library(knitr)
library(janitor)

# Modeling packages
library(caret)    # for classification and regression training 
library(kernlab)  # for fitting SVMs
library(e1071)    # for fitting SVMs
library(keras) #modeling NN
library(recipes) #for eficient preprocessing
library(yardstick) #Tidy methods for measuring model performance
library(corrr) #for Correlation Analysis
library(readr)

knitr::opts_chunk$set(
  tidy = FALSE,   # display code as typed
  size = "small") # slightly smaller font for code
```


# Creating dataset

First we read in the data. 
```{r}
movies <- fread('ml-25m/movies.csv')

features_fromPart2 <- readRDS('features_fromPart2.rds', refhook = NULL)

ratings <- fread('ml-25m/ratings.csv')

```

Only need to run this if we're using a subset of movies.

```{r}
movies <- movies %>%
  filter(movieId %in% features_fromPart2$movieId)
#movies <- movies[sample(nrow(movies), 0.1*nrow(movies)), ]

ratings <- ratings %>%
  filter(movieId %in% movies$movieId)

```



Removing duplicates in the movies dataset

```{r}
repeatMovies <- names(which(table(movies$title) > 1))
if (length(repeatMovies)  > 0 ) {
removeRows <- integer()
for(i in repeatMovies){
  repeatMovieLoc <- which(movies$title == i)
  tempGenre <- paste(movies$genres[repeatMovieLoc], collapse="|")
  tempGenre <- paste(unique(unlist(strsplit(tempGenre, split = "\\|")[[1]])), collapse = "|")
  movies$genres[repeatMovieLoc[1]] <- tempGenre
  
  ##### REMOVE REPEATS IN RATING DATA ####
  repeatMovieIdLoc <- which(ratings$movieId %in% movies$movieId[repeatMovieLoc[-1]])
  ratings$movieId[repeatMovieIdLoc] <- movies$movieId[repeatMovieLoc[1]]
  removeRows <- c(removeRows, repeatMovieLoc[-1])
}
movies$movieId[removeRows]
movies <- movies[-removeRows,]
rm(i, removeRows, repeatMovieIdLoc, repeatMovieLoc, repeatMovies, tempGenre)} 
```


Next, we need to remove any user that has rated a single movie twice.
We only keep the highest rating if they have rated a single movie twice. 
```{r}
duplicate_ratings_keep <- ratings %>% 
  get_dupes(userId, movieId) %>%
  group_by(userId, movieId) %>%
  summarise(rating = max(rating)) 
  
ratings <- ratings %>%
  # completely removing all duplicates
  group_by(userId, movieId) %>% 
  filter(n()==1) %>%
  # adding the desired duplicates back in
  left_join(duplicate_ratings_keep)
```


We get the average rating per movie and the genre dummies for the movie
```{r}
# getting the average rating per movie
av_rating <- ratings %>%
  group_by(movieId) %>%
  summarise(av_rating = mean(rating)) 

rm(ratings)

# generating the dummy variables for the genres
genres_df <- movies %>%
  #splitting the genre string into a list of genres
  mutate(genres = strsplit(genres, split = "\\|")) %>%
  # creating a long dataframe with each row being 1 genre for each movie
  unnest(genres) %>%
  
  # preparing to pivot_wider such that each genre is a column
  # adding the value = 1 so that the value is 1 where that genre is in the movie
  mutate(value = 1) %>%
  pivot_wider(names_from = 'genres') %>%
  # replacing NAs with zero to finish the dummy variables
  replace(is.na(.), 0)# %>%
  
  # only 1 movie has the genre 'film noir' so we remove it entirely
 # select(-`Film-Noir`)

```

Adding all features together to create one dataframe we can use for the tasks following. 
```{r}
df <- left_join(av_rating, genres_df, by = 'movieId') %>%
  left_join(features_fromPart2, by = 'movieId')
```


# Training SVM model

To start with, we want to predict whether a movie gets an excellent rating, which is defined as a rating above 3.75. Therefore, we create a binary variable y. This is the variable we will try to predict in our SVM. Before we can train our model on the data, we need to split it into a training and test set. 

```{r}
# binarizing the data to make it a classification problem
svm_df <- df %>%
  mutate(y = if_else(av_rating > 3.75, 1, 0),
         y = as.factor(y)) %>%
  select(-av_rating, -title)

# splitting the data
set.seed(42)  # for reproducibility
splitting <- initial_split(svm_df, prop = 0.8)
train <- training(splitting)
test  <- testing(splitting)

```


Next, we train our model on the trainingset. We have picked a radial kernel, and we cross-validate our data. We're tuning with 10 default values. 

```{r}
grid <- expand.grid(sigma = seq(0.03,0.05,0.002),
                    C = seq(1, 10, 1))
svm_tune <- train(
  y ~ ., 
  data = train,
  method = "svmRadial",         # Radial kernel      
  preProcess = c("center", "scale"),  # center & scale the data
  trControl = trainControl(method = "cv", number = 10), #cross-validation (10-fold) 
  tuneGrid = grid,
  metric = 'Kappa'
)
```

To assess the performance of our model, we plot the outcome. 

```{r}

ggplot(svm_tune) + theme_light()
print(svm_tune)
#it looks like C=7, \sigma = 0.016 give the best accuracy
#dispay and plot results of cross validation
confusionMatrix(svm_tune)

```



```{r}

#Model validation on the test set
test_validation = predict(svm_tune, test) 
confusionMatrix(data = test_validation, as.factor(test$y))

```

To know how well our SVM performs relatively, we need to compare it to a simpler classification model. In this case, we compare it to a logistic regression, see below. We use the same tuning system and optimize it accordingly.

```{r}
grid <- expand.grid(alpha = seq(0.1, 1, 0.1), # x
                    lambda = seq(0.01,0.05,0.01)) # colored lines
log_tune <- train(
  y ~ ., 
  data = train,
  method = "glmnet",         # Radial kernel      
  preProcess = c("center", "scale"),  # center & scale the data
  trControl = trainControl(method = "cv", number = 10), #cross-validation (10-fold) 
  tuneGrid = grid,
  metric = 'Kappa'
)

ggplot(log_tune) + theme_light()
#print(log_tune)
#it looks like C=7, \sigma = 0.016 give the best accuracy
#dispay and plot results of cross validation
#confusionMatrix(log_tune)

#Model validation on the test set
test_validation = predict(log_tune, test) 
confusionMatrix(data = test_validation, as.factor(test$y))

```

With this information we can compare the two. Before we do that however, we need to decide what metric is most important to us, which depends on the use-case. For the purpose of this exercise we are going to be looking for a movie to watch and if we get a new movie, we want to predict which one is going to be good and therefore which one we should watch. We do not want to waste our time watching a bad movie and we do not really care that we will not watch all good movies, there are too many of those anyway. Therefore, the measure we care about is specificity. We see that the logarithmic model has a specificity of 0.39, while the SVM has a specificity of 0.41. This is a minimal difference and not worth the additional complication that the SVM model brings, which increases the likelihood of overfitting. 



# Training an ANN model

Knowing whether a movie would be rated good or bad can be useful to decide if you want to watch it, but we need more detailed information to make any more complicated decisions such as whether to add it to your streaming service. To provide this information, we will be building an Artificial Neural Network. This network will predict the exact average rating that the movie will get. 

```{r}
# removing the unnecessary title column
ann_df <- df %>%
  select( -title, -movieId, -`(no genres listed)` ) 

# splitting the data
set.seed(42)  # for reproducibility
splitting <- initial_split(ann_df, prop = 0.8)
train <- training(splitting)
test  <- testing(splitting) 
```

```{r}
ann_df %>%
  pivot_longer(cols=c(3:22), names_to ='genre', values_to = 'nr') %>%
  group_by(genre) %>%
  summarise(n = sum(nr))
```


```{r}
# since we have previously preprocessed the data,
# we only need to center and scale it before we can use it
rec_obj <- recipe(av_rating ~ ., data = train) %>%
  step_center(all_predictors(), -all_outcomes()) %>%
  step_scale(all_predictors(), -all_outcomes()) %>%
  prep(data = train)
```

Next, we're creating the X and Y sets.
```{r}
# Creating the X and Y sets
x_train_tbl <- bake(rec_obj, new_data = train) %>% select(-av_rating)
x_test_tbl  <- bake(rec_obj, new_data = test) %>% select(-av_rating)
#x_train_tbl <- train %>% select(-av_rating)
#x_test_tbl <- test %>% select(-av_rating)
glimpse(x_train_tbl)
y_train_vec <- train$av_rating
y_test_vec  <- test$av_rating
```

```{r}
# NOTE FOR VIVIAN: TO RUN THIS YOU NEED TO RUN THE FOLLOWING IN THE CONSOLE FIRST: 
# library(reticulate)
# conda_list() --> find the spyder2022 version of python
# use_python('path of spyder2022 version of python')
# use_condaenv('spyder2022')
# then and only then will keras run correctly. DO NOT FORGET

# Building our Artificial Neural Network
model_keras <- keras_model_sequential()

model_keras %>% 
  
  # First hidden layer
  layer_dense(
    units              = 16, 
    kernel_initializer = "uniform", 
    activation         = "tanh", 
    input_shape        = ncol(x_train_tbl)) %>% 
  
  # Dropout to prevent overfitting
  layer_dropout(rate = 0.1) %>%
  
  # Second hidden layer
  layer_dense(
    units              = 16, 
    kernel_initializer = "uniform", 
    activation         = "tanh") %>% 
  
  # Dropout to prevent overfitting
  layer_dropout(rate = 0.1) %>%
  
   # Third hidden layer
  layer_dense(
    units              = 16, 
    kernel_initializer = "uniform", 
    activation         = "linear") %>% 
  
  # Dropout to prevent overfitting
  layer_dropout(rate = 0.1) %>%
  
  # Output layer
  layer_dense(
    units              = 1, #For multi-classification, the units should correspond to the number of classes
    kernel_initializer = "uniform", 
    activation         = "linear") %>% 
  
  # Compile NN
  compile(
    optimizer = 'adam',
    loss='mse',
    metrics= list('mean_squared_error')
  )

#display model architecture
model_keras
```

```{r}
# Train model
history <- fit(
  object           = model_keras, 
  x                = as.matrix(x_train_tbl), 
  y                = y_train_vec,
  batch_size       = 50, 
  epochs           = 50,
  validation_split = 0.30 #to include 30% of the data for model validation, which prevents overfitting.
 # verbose          = 0
)

```

```{r}
# Print a summary of the training history
print(history)
```

```{r}
# Plot the training/validation history of our Keras model
plot(history)
```


Next, we're evaluating the model:
```{r}
scores = model_keras %>% evaluate(as.matrix(x_test_tbl), y_test_vec, verbose = 0)
print(scores)

y_test_pred = model_keras %>% 
  predict(as.matrix(x_test_tbl)) %>% 
  as.vector()
R2 = cor(y_test_vec,y_test_pred)^2
R2

y_train_pred = model_keras %>% 
  predict(as.matrix(x_train_tbl)) %>%
  as.vector()
R2 = cor(y_train_vec, y_train_pred)^2
R2
```

```{r}
saveRDS(df, file = "part3_df_object.rds")
```



